# MSE Decomposition Analysis: CISIR vs CISIR w/o wPCC

**Analysis Date:** November 4, 2025  
**Seeds Analyzed:** 5 (456789, 42, 123, 0, 9999)  
**Test Samples:** 11,839 total (112 rare events, 0.9%)  
**Rare Event Threshold:** Î´ â‰¥ 0.5

---

## Executive Summary

This analysis compares two SEP-EC forecasting models using Murphy (1988) MSE decomposition:
- **CISIR w/o wPCC**: Trained with MSE loss only (Î»_pcc = 0)
- **CISIR**: Trained with MSE + wPCC regularizer (Î»_pcc > 0)

**Key Finding:** Including wPCC regularization reduces overall MSE by **97.0%** on all samples and **84.4%** on rare events, with the most dramatic improvement in the standard deviation mismatch term (~100% reduction).

---

## Main Results: Averaged Across 5 Seeds

### Summary Table

| Test Subset | Model | Mean Term | SD Term | Corr Term | **Total MSE** |
|-------------|-------|-----------|---------|-----------|---------------|
| **All Samples** | CISIR w/o wPCC | 0.1445 | 1.1987 | 0.3041 | **1.647** |
| **All Samples** | CISIR (w/ wPCC) | 0.0241 | 0.0004 | 0.0255 | **0.050** |
| **Rare Events** | CISIR w/o wPCC | 0.3662 | 0.7928 | 0.3705 | **1.530** |
| **Rare Events** | CISIR (w/ wPCC) | 0.1494 | 0.0277 | 0.0620 | **0.239** |

### Changes with wPCC (Î” and %Î”)

| Test Subset | Term | CISIR w/o wPCC | CISIR | **Î”** | **%Î”** |
|-------------|------|----------------|-------|-------|--------|
| **All** | Mean: (È³ - È³Ì‚)Â² | 0.1445 | 0.0241 | **-0.1204** | **-83.3%** |
| **All** | SD: (sd(Å·) - sd(y))Â² | 1.1987 | 0.0004 | **-1.1982** | **-100.0%** â­ |
| **All** | Corr: 2Â·sd(Å·)Â·sd(y)Â·(1-PCC) | 0.3041 | 0.0255 | **-0.2787** | **-91.6%** |
| **Rare** | Mean: (È³ - È³Ì‚)Â² | 0.3662 | 0.1494 | **-0.2168** | **-59.2%** |
| **Rare** | SD: (sd(Å·) - sd(y))Â² | 0.7928 | 0.0277 | **-0.7651** | **-96.5%** â­ |
| **Rare** | Corr: 2Â·sd(Å·)Â·sd(y)Â·(1-PCC) | 0.3705 | 0.0620 | **-0.3085** | **-83.3%** |

---

## Detailed Breakdown by Term

### 1. Mean Mismatch Term: (È³ - È³Ì‚)Â²

**Measures:** Bias in predictions (difference between mean of predictions and mean of true values)

| Subset | CISIR w/o wPCC | CISIR | Improvement |
|--------|----------------|-------|-------------|
| All | 0.1445 | 0.0241 | **-83.3%** |
| Rare | 0.3662 | 0.1494 | **-59.2%** |

**Interpretation:** wPCC reduces prediction bias by ~60-83%, making predictions more centered around the true mean.

---

### 2. Standard Deviation Mismatch Term: (sd(Å·) - sd(y))Â² â­ MOST DRAMATIC

**Measures:** Over/under-dispersion of predictions (mismatch in prediction variability vs true variability)

| Subset | CISIR w/o wPCC | CISIR | Improvement |
|--------|----------------|-------|-------------|
| All | 1.1987 | 0.0004 | **~100%** |
| Rare | 0.7928 | 0.0277 | **-96.5%** |

**Key Insight:** 
- **Without wPCC:** Predictions have SD ~1.2-1.3 (true SD = 0.13) â†’ **10x over-dispersed!**
- **With wPCC:** Prediction SD matches true SD almost perfectly
- **This is the primary failure mode of MSE-only training:** Models make overly variable predictions

---

### 3. Correlation Deficit Term: 2Â·sd(Å·)Â·sd(y)Â·(1-PCC)

**Measures:** How well predictions correlate with true values (lower is better)

| Subset | CISIR w/o wPCC | CISIR | Improvement |
|--------|----------------|-------|-------------|
| All | 0.3041 | 0.0255 | **-91.6%** |
| Rare | 0.3705 | 0.0620 | **-83.3%** |

**Key Insight:**
- **Without wPCC:** PCC ~0.03-0.06 (almost no correlation)
- **With wPCC:** PCC ~0.28 for all samples, **~0.70 for rare events** ðŸŽ¯
- **wPCC directly addresses the target:** Improves correlation, which MSE alone ignores

---

## Overall MSE Reduction

### All Test Samples (n=11,839)
```
CISIR w/o wPCC: 1.647
CISIR (w/ wPCC): 0.050
Reduction: 97.0% â¬‡ï¸
```

### Rare Events (n=112, Î´ â‰¥ 0.5)
```
CISIR w/o wPCC: 1.530
CISIR (w/ wPCC): 0.239
Reduction: 84.4% â¬‡ï¸
```

---

## Individual Seed Results

### Seed 0
| Subset | Term | w/o wPCC | w/ wPCC | Î” | %Î” |
|--------|------|----------|---------|---|-----|
| All | Mean | 0.0138 | 0.0437 | +0.0298 | +215.5% |
| All | SD | 1.3125 | 0.0000 | -1.3125 | -100.0% |
| All | Corr | 0.3086 | 0.0239 | -0.2847 | -92.3% |
| Rare | Mean | 0.4614 | 0.0902 | -0.3712 | -80.5% |
| Rare | SD | 0.8973 | 0.0407 | -0.8566 | -95.5% |
| Rare | Corr | 0.3686 | 0.0700 | -0.2986 | -81.0% |

### Seed 42
| Subset | Term | w/o wPCC | w/ wPCC | Î” | %Î” |
|--------|------|----------|---------|---|-----|
| All | Mean | 0.3269 | 0.0185 | -0.3085 | -94.4% |
| All | SD | 1.1789 | 0.0007 | -1.1782 | -99.9% |
| All | Corr | 0.3042 | 0.0190 | -0.2852 | -93.8% |
| Rare | Mean | 0.0464 | 0.1842 | +0.1378 | +297.2% |
| Rare | SD | 0.7482 | 0.0186 | -0.7296 | -97.5% |
| Rare | Corr | 0.3894 | 0.0475 | -0.3419 | -87.8% |

### Seed 123
| Subset | Term | w/o wPCC | w/ wPCC | Î” | %Î” |
|--------|------|----------|---------|---|-----|
| All | Mean | 0.0262 | 0.0160 | -0.0101 | -38.8% |
| All | SD | 1.0808 | 0.0007 | -1.0802 | -99.9% |
| All | Corr | 0.2993 | 0.0298 | -0.2694 | -90.0% |
| Rare | Mean | 0.7775 | 0.1605 | -0.6170 | -79.4% |
| Rare | SD | 0.6447 | 0.0330 | -0.6117 | -94.9% |
| Rare | Corr | 0.3267 | 0.0650 | -0.2618 | -80.1% |

### Seed 9999
| Subset | Term | w/o wPCC | w/ wPCC | Î” | %Î” |
|--------|------|----------|---------|---|-----|
| All | Mean | 0.2695 | 0.0130 | -0.2565 | -95.2% |
| All | SD | 1.1408 | 0.0000 | -1.1407 | -100.0% |
| All | Corr | 0.3003 | 0.0253 | -0.2749 | -91.6% |
| Rare | Mean | 0.1392 | 0.1691 | +0.0299 | +21.5% |
| Rare | SD | 0.7882 | 0.0296 | -0.7586 | -96.2% |
| Rare | Corr | 0.3978 | 0.0708 | -0.3270 | -82.2% |

### Seed 456789
| Subset | Term | w/o wPCC | w/ wPCC | Î” | %Î” |
|--------|------|----------|---------|---|-----|
| All | Mean | 0.0862 | 0.0293 | -0.0569 | -66.0% |
| All | SD | 1.2803 | 0.0008 | -1.2795 | -99.9% |
| All | Corr | 0.3084 | 0.0293 | -0.2791 | -90.5% |
| Rare | Mean | 0.4065 | 0.1431 | -0.2633 | -64.8% |
| Rare | SD | 0.8857 | 0.0167 | -0.8691 | -98.1% |
| Rare | Corr | 0.3701 | 0.0566 | -0.3135 | -84.7% |

---

## LaTeX Table for Paper

### Complete Table (Ready for Copy-Paste)

```latex
\begin{table*}[t]

\caption{Effects of including $w\mathrm{PCC}$ in CISIR on the MSE decomposition terms for SEP--EC (cf. Eq.~\eqref{eq:bv-sd}).}

\label{tab:mse-decomp-sep-ec}

\centering

\small

\setlength{\tabcolsep}{4pt}

\begin{tabular}{lccccccccccccc}

\toprule

\multirow{2}{*}{Test Subset} &

\multicolumn{4}{c}{$\bigl(\bar{y}-\bar{\hat{y}}\bigr)^2$} &

\multicolumn{4}{c}{$\bigl(\operatorname{sd}(\hat{y})-\operatorname{sd}(y)\bigr)^2$} &

\multicolumn{4}{c}{$2\,\operatorname{sd}(\hat{y})\,\operatorname{sd}(y)\,\bigl(1-\operatorname{PCC}(\hat{y},y)\bigr)$} \\

\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}

& CISIR w/o $w\mathrm{PCC}$ & CISIR & \textbf{$\Delta$} & \textbf{\%$\Delta$} &

  CISIR w/o $w\mathrm{PCC}$ & CISIR & \textbf{$\Delta$} & \textbf{\%$\Delta$} &

  CISIR w/o $w\mathrm{PCC}$ & CISIR & \textbf{$\Delta$} & \textbf{\%$\Delta$} \\

\midrule

All  & 0.1445 & 0.0241 & \textbf{-0.1204} & \textbf{-83.3} & 
       1.1987 & 0.0004 & \textbf{-1.1982} & \textbf{-100.0} & 
       0.3041 & 0.0255 & \textbf{-0.2787} & \textbf{-91.6} \\

Rare & 0.3662 & 0.1494 & \textbf{-0.2168} & \textbf{-59.2} & 
       0.7928 & 0.0277 & \textbf{-0.7651} & \textbf{-96.5} & 
       0.3705 & 0.0620 & \textbf{-0.3085} & \textbf{-83.3} \\

\bottomrule

\end{tabular}

\end{table*}
```

### Just the Table Rows (if you already have the table structure)

```latex
All  & 0.1445 & 0.0241 & \textbf{-0.1204} & \textbf{-83.3} & 1.1987 & 0.0004 & \textbf{-1.1982} & \textbf{-100.0} & 0.3041 & 0.0255 & \textbf{-0.2787} & \textbf{-91.6} \\
Rare & 0.3662 & 0.1494 & \textbf{-0.2168} & \textbf{-59.2} & 0.7928 & 0.0277 & \textbf{-0.7651} & \textbf{-96.5} & 0.3705 & 0.0620 & \textbf{-0.3085} & \textbf{-83.3} \\
```

---

## Interpretation & Conclusions

### The Problem with MSE-Only Training (CISIR w/o wPCC)

1. **Over-dispersed predictions:** SD of predictions is ~10x larger than true SD
2. **Poor correlation:** PCC â‰ˆ 0.03-0.06 (essentially random)
3. **High total error:** MSE â‰ˆ 1.6-1.7

**Why?** MSE can be minimized by:
- Making the third term small (by reducing sd(Å·))
- But the second term penalizes this!
- Models get "stuck" making overly variable predictions
- This is the **classic MSE failure mode** described in your paper

### The Solution: wPCC Regularization (CISIR)

1. **Well-calibrated predictions:** SD matches true SD almost perfectly
2. **Strong correlation:** PCC â‰ˆ 0.28 overall, **0.70 for rare events** âœ…
3. **Low total error:** MSE â‰ˆ 0.05 (97% reduction)

**Why it works:** wPCC directly penalizes poor correlation (minimizes 1 - PCC), forcing the model to:
- Learn the actual relationships in the data
- Produce predictions that correlate with outcomes
- Avoid the over-dispersion trap

### Key Takeaway for Paper

> *Including wPCC regularization transforms model behavior from producing over-dispersed, poorly-correlated predictions to well-calibrated predictions with strong correlation. The standard deviation mismatch term shows the most dramatic improvement (~100% reduction), demonstrating that wPCC prevents the model from making overly variable predictionsâ€”a common failure mode when optimizing MSE alone. This validates our hypothesis that MSE's correlation-blindness is a critical limitation, and wPCC successfully addresses it.*

---

## Technical Notes

### Decomposition Formula (Murphy 1988)

```
MSE(Å·,y) = (È³ - È³Ì‚)Â² + (sd(Å·) - sd(y))Â² + 2Â·sd(Å·)Â·sd(y)Â·(1 - PCC(Å·,y))
            \_________/   \_______________/   \____________________________/
             Bias term      Dispersion term     Correlation deficit term
```

### Decomposition Error Analysis

Decomposition errors ranged from 0.000002 to 0.011, which are **excellent** and well within acceptable bounds:
- Maximum error: 0.011 (on rare events with MSE ~1.7)
- Relative error: < 0.65% of MSE value
- Due to numerical precision in floating-point arithmetic
- **Conclusion:** Decomposition is mathematically valid âœ…

### Hardware & Configuration

- **GPU:** NVIDIA A100-SXM4-40GB (37.4 GB available)
- **Framework:** TensorFlow 2.15.0 with CUDA support
- **Inference:** All predictions run on GPU for fast computation
- **Model Architecture:** MLP with residual connections [2048, 128, 1024, 128, 512, 128, 256, 128]
- **Output:** Dual-head (representation + forecast)

---

## Files Generated

1. **Detailed CSV:** `logs/mse_decomposition_detailed_20251104_173014.csv`
   - 24 rows: 5 seeds Ã— 2 subsets Ã— 2 models
   - Includes individual term values and total MSE for each combination

2. **Summary CSV:** `logs/mse_decomposition_summary_20251104_173014.csv`
   - 2 rows: All and Rare subsets (averaged across seeds)
   - Includes Î” and %Î” values for direct comparison

3. **Full Log:** `logs/decomp_output.txt`
   - Complete output with all intermediate computations
   - Useful for verification and debugging

---

## Contact & Reproducibility

**Script:** `load_get_decomps.py`  
**Command:** `python load_get_decomps.py`  
**Date:** November 4, 2025 17:30 UTC

All results are reproducible using the saved model checkpoints and the provided script.

---

*Generated by MSE Decomposition Analysis Pipeline*  
*NeurIPS 2025 Paper Submission*

