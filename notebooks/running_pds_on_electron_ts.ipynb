{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from ts_modeling import build_dataset, build_full_dataset, create_mlp, evaluate_model, process_sep_events\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from wandb.keras import WandbCallback\n",
    "from evaluate.utils import plot_tsne_pds\n",
    "from models import modeling\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:42:48.573349200Z",
     "start_time": "2024-02-01T17:42:48.540850300Z"
    }
   },
   "id": "9b940281c77235f3",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# SEEDING\n",
    "SEED = 42  # seed number \n",
    "\n",
    "# Set NumPy seed\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set TensorFlow seed\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Set random seed\n",
    "random.seed(SEED)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T16:21:18.097780900Z",
     "start_time": "2024-01-26T16:21:18.037780100Z"
    }
   },
   "id": "f76875d8faf6e350",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mb = modeling.ModelBuilder()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:50:08.609532100Z",
     "start_time": "2024-02-01T17:50:04.624035200Z"
    }
   },
   "id": "f3e6c466b78f6b70",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33merud1t3\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>D:\\College\\Fall2023\\sep-forecasting-research\\notebooks\\wandb\\run-20240201_130538-jica9cxa</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/erud1t3/mlp-ts-pds/runs/jica9cxa' target=\"_blank\">MLP_e0_5_e1_8_add_slope_True_20240201-130532</a></strong> to <a href='https://wandb.ai/erud1t3/mlp-ts-pds' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/erud1t3/mlp-ts-pds' target=\"_blank\">https://wandb.ai/erud1t3/mlp-ts-pds</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/erud1t3/mlp-ts-pds/runs/jica9cxa' target=\"_blank\">https://wandb.ai/erud1t3/mlp-ts-pds/runs/jica9cxa</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (24217, 98, 1)\n",
      "y_train.shape: (24217,)\n",
      "X_subtrain.shape: (18952, 98, 1)\n",
      "y_subtrain.shape: (18952,)\n",
      "X_test.shape: (10357, 98, 1)\n",
      "y_test.shape: (10357,)\n",
      "X_val.shape: (5265, 98, 1)\n",
      "y_val.shape: (5265,)\n",
      "X_train[0]: [[ 8.32918127e-01]\n",
      " [ 8.32441929e-01]\n",
      " [ 8.31286513e-01]\n",
      " [ 8.30311912e-01]\n",
      " [ 8.27953573e-01]\n",
      " [ 8.29726848e-01]\n",
      " [ 8.29137084e-01]\n",
      " [ 8.22263682e-01]\n",
      " [ 8.19803034e-01]\n",
      " [ 8.19114328e-01]\n",
      " [ 8.17616930e-01]\n",
      " [ 8.18847247e-01]\n",
      " [ 8.18795562e-01]\n",
      " [ 8.18419663e-01]\n",
      " [ 8.16262466e-01]\n",
      " [ 8.17023541e-01]\n",
      " [ 8.15991693e-01]\n",
      " [ 8.15499170e-01]\n",
      " [ 8.15388439e-01]\n",
      " [ 8.15226785e-01]\n",
      " [ 8.13570890e-01]\n",
      " [ 8.11499156e-01]\n",
      " [ 8.13348330e-01]\n",
      " [ 8.14014860e-01]\n",
      " [ 8.11950317e-01]\n",
      " [ 4.84655267e-01]\n",
      " [ 4.78858197e-01]\n",
      " [ 4.78858197e-01]\n",
      " [ 4.76140841e-01]\n",
      " [ 4.73112523e-01]\n",
      " [ 4.73982469e-01]\n",
      " [ 4.71807092e-01]\n",
      " [ 4.57714143e-01]\n",
      " [ 4.43027860e-01]\n",
      " [ 4.48199824e-01]\n",
      " [ 4.51296118e-01]\n",
      " [ 4.39538095e-01]\n",
      " [ 4.52987532e-01]\n",
      " [ 4.43815616e-01]\n",
      " [ 4.42870870e-01]\n",
      " [ 4.42236379e-01]\n",
      " [ 4.41446991e-01]\n",
      " [ 4.40015322e-01]\n",
      " [ 4.33586708e-01]\n",
      " [ 4.31145357e-01]\n",
      " [ 4.44132539e-01]\n",
      " [ 4.31634195e-01]\n",
      " [ 4.32609821e-01]\n",
      " [ 4.36171200e-01]\n",
      " [ 4.25213008e-01]\n",
      " [-4.76197298e-04]\n",
      " [-1.15541633e-03]\n",
      " [-9.74601191e-04]\n",
      " [-2.35833901e-03]\n",
      " [ 1.77327558e-03]\n",
      " [-5.89764381e-04]\n",
      " [-6.87340200e-03]\n",
      " [-2.46064772e-03]\n",
      " [-6.88706217e-04]\n",
      " [-1.49739846e-03]\n",
      " [ 1.23031739e-03]\n",
      " [-5.16846764e-05]\n",
      " [-3.75899386e-04]\n",
      " [-2.15719670e-03]\n",
      " [ 7.61075020e-04]\n",
      " [-1.03184874e-03]\n",
      " [-4.92522158e-04]\n",
      " [-1.10731498e-04]\n",
      " [-1.61654380e-04]\n",
      " [-1.65589447e-03]\n",
      " [-2.07173390e-03]\n",
      " [ 1.84917337e-03]\n",
      " [ 6.66530459e-04]\n",
      " [-2.06454297e-03]\n",
      " [-5.79706997e-03]\n",
      " [ 0.00000000e+00]\n",
      " [-2.71735578e-03]\n",
      " [-3.02831795e-03]\n",
      " [ 8.69945786e-04]\n",
      " [-2.17537679e-03]\n",
      " [-1.40929492e-02]\n",
      " [-1.46862831e-02]\n",
      " [ 5.17196466e-03]\n",
      " [ 3.09629347e-03]\n",
      " [-1.17580223e-02]\n",
      " [ 1.34494371e-02]\n",
      " [-9.17191668e-03]\n",
      " [-9.44745670e-04]\n",
      " [-6.34491561e-04]\n",
      " [-7.89387670e-04]\n",
      " [-1.43166882e-03]\n",
      " [-6.42861421e-03]\n",
      " [-2.44135111e-03]\n",
      " [ 1.29871821e-02]\n",
      " [-1.24983440e-02]\n",
      " [ 9.75626014e-04]\n",
      " [ 3.56137911e-03]\n",
      " [-1.09581922e-02]]\n",
      "y_train[0]: 0.7607123667223692\n",
      "n_features: 98\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 98)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               9900      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 50)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 9)                 459       \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 9)                 0         \n",
      "                                                                 \n",
      " normalize_layer (NormalizeL  (None, 9)                0         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,509\n",
      "Trainable params: 25,509\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10000\n",
      "213/593 [=========>....................] - ETA: 8:55 - loss: 11.8695"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inputs_to_use in [['e0.5', 'e1.8'], ['e0.5', 'e1.8', 'p'], ['e0.5'], ['e0.5', 'p']]:\n",
    "    for add_slope in [True, False]:\n",
    "\n",
    "        # PARAMS\n",
    "        # inputs_to_use = ['e0.5']\n",
    "        # add_slope = True\n",
    "\n",
    "        # Join the inputs_to_use list into a string, replace '.' with '_', and join with '-'\n",
    "        inputs_str = \"_\".join(input_type.replace('.', '_') for input_type in inputs_to_use)\n",
    "\n",
    "        # Construct the title\n",
    "        title = f'MLP_{inputs_str}_add_slope_{str(add_slope)}'\n",
    "\n",
    "        # Replace any other characters that are not suitable for filenames (if any)\n",
    "        title = title.replace(' ', '_').replace(':', '_')\n",
    "\n",
    "        # Create a unique experiment name with a timestamp\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        experiment_name = f'{title}_{current_time}'\n",
    "\n",
    "        # Initialize wandb\n",
    "        wandb.init(project=\"mlp-ts-pds\", name=experiment_name, config={\n",
    "            \"inputs_to_use\": inputs_to_use,\n",
    "            \"add_slope\": add_slope,\n",
    "        })\n",
    "\n",
    "        # set the root directory\n",
    "        root_dir = 'D:/College/Fall2023/electron_cme_v4/electron_cme_data_split'\n",
    "        # build the dataset\n",
    "        X_train, y_train = build_dataset(root_dir + '/training', inputs_to_use=inputs_to_use, add_slope=add_slope)\n",
    "        X_subtrain, y_subtrain = build_dataset(root_dir + '/subtraining', inputs_to_use=inputs_to_use,\n",
    "                                               add_slope=add_slope)\n",
    "        X_test, y_test = build_dataset(root_dir + '/testing', inputs_to_use=inputs_to_use, add_slope=add_slope)\n",
    "        X_val, y_val = build_dataset(root_dir + '/validation', inputs_to_use=inputs_to_use, add_slope=add_slope)\n",
    "\n",
    "        # print all data shapes\n",
    "        print(f'X_train.shape: {X_train.shape}')\n",
    "        print(f'y_train.shape: {y_train.shape}')\n",
    "        print(f'X_subtrain.shape: {X_subtrain.shape}')\n",
    "        print(f'y_subtrain.shape: {y_subtrain.shape}')\n",
    "        print(f'X_test.shape: {X_test.shape}')\n",
    "        print(f'y_test.shape: {y_test.shape}')\n",
    "        print(f'X_val.shape: {X_val.shape}')\n",
    "        print(f'y_val.shape: {y_val.shape}')\n",
    "\n",
    "        # print a sample of the training data\n",
    "        print(f'X_train[0]: {X_train[0]}')\n",
    "        print(f'y_train[0]: {y_train[0]}')\n",
    "\n",
    "        # get the number of features\n",
    "        n_features = X_train.shape[1]\n",
    "        print(f'n_features: {n_features}')\n",
    "        hiddens = [100, 100, 50]\n",
    "\n",
    "        # create the model\n",
    "        # mlp_model_sep = create_mlp(input_dim=n_features, hiddens=hiddens)\n",
    "        mlp_model_sep = mb.create_model_pds(input_dim=n_features, hiddens=hiddens, feat_dim=9)\n",
    "        mlp_model_sep.summary()\n",
    "\n",
    "        # Set the early stopping patience and learning rate as variables\n",
    "        Options = {\n",
    "            'batch_size': 32,  # Assuming batch_size is defined elsewhere\n",
    "            'epochs': 10000,\n",
    "            'patience': 50,  # Updated to 50\n",
    "            'learning_rate': 3e-4,  # Updated to 3e-4\n",
    "            'weight_decay': 0,  # Added weight decay\n",
    "            'momentum_beta1': 0.9,  # Added momentum beta1\n",
    "        }\n",
    "\n",
    "\n",
    "        # Define the EarlyStopping callback\n",
    "        # early_stopping = EarlyStopping(monitor='val_forecast_head_loss', patience=patience, verbose=1,\n",
    "        #                                restore_best_weights=True)\n",
    "\n",
    "        # Compile the model with the specified learning rate\n",
    "        # mlp_model_sep.compile(optimizer=Adam(learning_rate=learning_rate,\n",
    "        #                                       weight_decay=weight_decay,\n",
    "        #                                       beta_1=momentum_beta1),\n",
    "        #                       loss={'forecast_head': 'mse'})\n",
    "\n",
    "        # Train the model with the callback\n",
    "        # history = mlp_model_sep.fit(X_subtrain,\n",
    "        #                             {'forecast_head': y_subtrain},\n",
    "        #                             epochs=1000, batch_size=32,\n",
    "        #                             validation_data=(X_val, {'forecast_head': y_val}),\n",
    "        #                             callbacks=[early_stopping, WandbCallback()])\n",
    "\n",
    "        # Plot the training and validation loss\n",
    "        # plt.figure(figsize=(12, 6))\n",
    "        # plt.plot(history.history['loss'], label='Training Loss')\n",
    "        # plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        # plt.title('Training and Validation Loss')\n",
    "        # plt.xlabel('Epochs')\n",
    "        # plt.ylabel('Loss')\n",
    "        # plt.legend()\n",
    "        # # save the plot\n",
    "        # plt.savefig(f'mlp_loss_{title}.png')\n",
    "\n",
    "        # Determine the optimal number of epochs from early stopping\n",
    "        # optimal_epochs = early_stopping.stopped_epoch - patience + 1  # Adjust for the offset\n",
    "        # final_mlp_model_sep = create_mlp(input_dim=n_features,\n",
    "        #                                  hiddens=hiddens)  # Recreate the model architecture\n",
    "        # final_mlp_model_sep.compile(optimizer=Adam(learning_rate=learning_rate,\n",
    "        #                                             weight_decay=weight_decay,\n",
    "        #                                             beta_1=momentum_beta1),\n",
    "        #                             loss={'forecast_head': 'mse'})  # Compile the model just like before\n",
    "        # # Train on the full dataset\n",
    "        # final_mlp_model_sep.fit(X_train, {'forecast_head': y_train}, epochs=optimal_epochs, batch_size=32,\n",
    "        #                         verbose=1)\n",
    "\n",
    "        # evaluate the model on test data\n",
    "        # error_mae = evaluate_model(final_mlp_model_sep, X_test, y_test)\n",
    "        # print(f'mae error: {error_mae}')\n",
    "        # # Log the MAE error to wandb\n",
    "        # wandb.log({\"mae_error\": error_mae})\n",
    "\n",
    "        # Process SEP event files in the specified directory\n",
    "        # test_directory = root_dir + '/testing'\n",
    "        # filenames = process_sep_events(\n",
    "        #     test_directory,\n",
    "        #     final_mlp_model_sep,\n",
    "        #     model_type='mlp',\n",
    "        #     title=title,\n",
    "        #     inputs_to_use=inputs_to_use,\n",
    "        #     add_slope=add_slope)\n",
    "\n",
    "        # Log the plot to wandb\n",
    "        # for filename in filenames:\n",
    "        #     wandb.log({f'{filename}': wandb.Image(filename)})\n",
    "        \n",
    "        mb.train_pds(mlp_model_sep,\n",
    "             X_subtrain, y_subtrain,\n",
    "             X_val, y_val,\n",
    "             X_train, y_train,\n",
    "             learning_rate=Options['learning_rate'],\n",
    "             epochs=Options['epochs'],\n",
    "             batch_size=Options['batch_size'],\n",
    "             patience=Options['patience'], save_tag=current_time + \"_features\")\n",
    "\n",
    "        # Log model to Weights & Biases\n",
    "        wandb.log_artifact('path/to/model', type='model', name='pds_model')\n",
    "        \n",
    "        file_path = plot_tsne_pds(mlp_model_sep,\n",
    "                                  X_train,\n",
    "                                  y_train,\n",
    "                                  title, 'training',\n",
    "                                  save_tag=current_time)\n",
    "        \n",
    "        # Log t-SNE plot for training\n",
    "        wandb.log_artifact(file_path, type='plot', name='tsne_training_plot')\n",
    "        print('file_path: ' + file_path)\n",
    "        \n",
    "        file_path = plot_tsne_pds(mlp_model_sep,\n",
    "                                  X_test,\n",
    "                                  y_test,\n",
    "                                  title, 'testing',\n",
    "                                  save_tag=current_time)\n",
    "        \n",
    "        # Log t-SNE plot for testing\n",
    "        wandb.log_artifact(file_path, type='plot', name='tsne_testing_plot')\n",
    "        print('file_path: ' + file_path)\n",
    "\n",
    "        # Finish the wandb run\n",
    "        wandb.finish()\n",
    "\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T18:11:05.140398400Z",
     "start_time": "2024-02-01T18:05:32.600900900Z"
    }
   },
   "id": "4ffd238878bf2115",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for inputs_to_use in [['e0.5', 'e1.8'], ['e0.5', 'e1.8', 'p']]:\n",
    "    for add_slope in [True, False]:\n",
    "        for cme_speed_threshold in [0, 500]:\n",
    "            # PARAMS\n",
    "            # inputs_to_use = ['e0.5']\n",
    "            # add_slope = True\n",
    "    \n",
    "            # Join the inputs_to_use list into a string, replace '.' with '_', and join with '-'\n",
    "            inputs_str = \"_\".join(input_type.replace('.', '_') for input_type in inputs_to_use)\n",
    "    \n",
    "            # Construct the title\n",
    "            title = f'MLP_{inputs_str}_add_slope_{str(add_slope)}'\n",
    "    \n",
    "            # Replace any other characters that are not suitable for filenames (if any)\n",
    "            title = title.replace(' ', '_').replace(':', '_')\n",
    "    \n",
    "            # Create a unique experiment name with a timestamp\n",
    "            current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            experiment_name = f'{title}_{current_time}'\n",
    "    \n",
    "            # Initialize wandb\n",
    "            wandb.init(project=\"mlp-ts-lowerlr\", name=experiment_name, config={\n",
    "                \"inputs_to_use\": inputs_to_use,\n",
    "                \"add_slope\": add_slope,\n",
    "            })\n",
    "    \n",
    "            # set the root directory\n",
    "            root_dir = 'D:/College/Fall2023/electron_cme_v4/electron_cme_data_split'\n",
    "            # build the dataset\n",
    "            X_train, y_train = build_dataset(root_dir + '/training', inputs_to_use=inputs_to_use, add_slope=add_slope)\n",
    "            X_subtrain, y_subtrain = build_dataset(root_dir + '/subtraining', inputs_to_use=inputs_to_use,\n",
    "                                                   add_slope=add_slope)\n",
    "            X_test, y_test = build_dataset(root_dir + '/testing', inputs_to_use=inputs_to_use, add_slope=add_slope)\n",
    "            X_val, y_val = build_dataset(root_dir + '/validation', inputs_to_use=inputs_to_use, add_slope=add_slope)\n",
    "    \n",
    "            # print all data shapes\n",
    "            print(f'X_train.shape: {X_train.shape}')\n",
    "            print(f'y_train.shape: {y_train.shape}')\n",
    "            print(f'X_subtrain.shape: {X_subtrain.shape}')\n",
    "            print(f'y_subtrain.shape: {y_subtrain.shape}')\n",
    "            print(f'X_test.shape: {X_test.shape}')\n",
    "            print(f'y_test.shape: {y_test.shape}')\n",
    "            print(f'X_val.shape: {X_val.shape}')\n",
    "            print(f'y_val.shape: {y_val.shape}')\n",
    "    \n",
    "            # print a sample of the training data\n",
    "            print(f'X_train[0]: {X_train[0]}')\n",
    "            print(f'y_train[0]: {y_train[0]}')\n",
    "    \n",
    "            # get the number of features\n",
    "            n_features = X_train.shape[1]\n",
    "            print(f'n_features: {n_features}')\n",
    "            hiddens = [100, 100, 50]\n",
    "    \n",
    "            # create the model\n",
    "            # mlp_model_sep = create_mlp(input_dim=n_features, hiddens=hiddens)\n",
    "            mlp_model_sep = modeling.create_mlp(input_dim=n_features, hiddens=hiddens)\n",
    "            mlp_model_sep.summary()\n",
    "    \n",
    "            # Set the early stopping patience and learning rate as variables\n",
    "            patience = 50\n",
    "            learning_rate = 3e-5\n",
    "            weight_decay = 0 # higher weight decay\n",
    "            momentum_beta1 = 0.9 # higher momentum beta1\n",
    "    \n",
    "            # Define the EarlyStopping callback\n",
    "            early_stopping = EarlyStopping(monitor='val_forecast_head_loss', patience=patience, verbose=1,\n",
    "                                           restore_best_weights=True)\n",
    "    \n",
    "            # Compile the model with the specified learning rate\n",
    "            mlp_model_sep.compile(optimizer=Adam(learning_rate=learning_rate,\n",
    "                                                  weight_decay=weight_decay,\n",
    "                                                  beta_1=momentum_beta1),\n",
    "                                  loss={'forecast_head': 'mse'})\n",
    "    \n",
    "            # Train the model with the callback\n",
    "            history = mlp_model_sep.fit(X_subtrain,\n",
    "                                        {'forecast_head': y_subtrain},\n",
    "                                        epochs=1000, batch_size=32,\n",
    "                                        validation_data=(X_val, {'forecast_head': y_val}),\n",
    "                                        callbacks=[early_stopping, WandbCallback()])\n",
    "    \n",
    "            # Plot the training and validation loss\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(history.history['loss'], label='Training Loss')\n",
    "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "            plt.title('Training and Validation Loss')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            # save the plot\n",
    "            plt.savefig(f'mlp_loss_{title}.png')\n",
    "    \n",
    "            # Determine the optimal number of epochs from early stopping\n",
    "            optimal_epochs = early_stopping.stopped_epoch - patience + 1  # Adjust for the offset\n",
    "            final_mlp_model_sep = create_mlp(input_dim=n_features,\n",
    "                                             hiddens=hiddens)  # Recreate the model architecture\n",
    "            final_mlp_model_sep.compile(optimizer=Adam(learning_rate=learning_rate,\n",
    "                                                        weight_decay=weight_decay,\n",
    "                                                        beta_1=momentum_beta1),\n",
    "                                        loss={'forecast_head': 'mse'})  # Compile the model just like before\n",
    "            # Train on the full dataset\n",
    "            final_mlp_model_sep.fit(X_train, {'forecast_head': y_train}, epochs=optimal_epochs, batch_size=32,\n",
    "                                    verbose=1)\n",
    "    \n",
    "            # evaluate the model on test data\n",
    "            error_mae = evaluate_model(final_mlp_model_sep, X_test, y_test)\n",
    "            print(f'mae error: {error_mae}')\n",
    "            # Log the MAE error to wandb\n",
    "            wandb.log({\"mae_error\": error_mae})\n",
    "    \n",
    "            # Process SEP event files in the specified directory\n",
    "            test_directory = root_dir + '/testing'\n",
    "            filenames = process_sep_events(\n",
    "                test_directory,\n",
    "                final_mlp_model_sep,\n",
    "                model_type='mlp',\n",
    "                title=title,\n",
    "                inputs_to_use=inputs_to_use,\n",
    "                add_slope=add_slope)\n",
    "    \n",
    "            # Log the plot to wandb\n",
    "            for filename in filenames:\n",
    "                wandb.log({f'{filename}': wandb.Image(filename)})\n",
    "    \n",
    "            # Finish the wandb run\n",
    "            wandb.finish()\n",
    "    \n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "805d14319f3bee0f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
