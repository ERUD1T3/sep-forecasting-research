{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T18:56:36.603639Z",
     "start_time": "2024-07-23T18:56:23.142438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.models import Model"
   ],
   "id": "4d72e93d61d544db",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_dataset(n_points: int) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset based on the given rules.\n",
    "    If x2 is negative, y = 2 * x1.\n",
    "    If x2 is positive, y = x1 + x2.\n",
    "\n",
    "    Args:\n",
    "        n_points (int): Number of data points to generate.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: Generated features and labels.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    x1 = np.random.randint(1, 10, size=n_points)\n",
    "    x2 = np.random.randint(-5, 6, size=n_points)\n",
    "    y = np.array([2 * x1[i] if x2[i] < 0 else x1[i] + x2[i] for i in range(n_points)])\n",
    "    return np.stack((x1, x2), axis=1), y\n",
    "\n",
    "\n",
    "# Initial given data\n",
    "initial_x = np.array([[1, -1], [2, 1], [3, -3], [4, 5]])\n",
    "initial_y = np.array([2, 3, 6, 9])\n",
    "\n",
    "# Generate additional data points\n",
    "x_additional, y_additional = generate_dataset(10)\n",
    "\n",
    "# Combine initial and additional data\n",
    "x = np.vstack((initial_x, x_additional))\n",
    "y = np.concatenate((initial_y, y_additional))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Dataset X:\\n\", x_train)\n",
    "print(\"Training Dataset Y:\\n\", y_train)\n",
    "print(\"Test Dataset X:\\n\", x_test)\n",
    "print(\"Test Dataset Y:\\n\", y_test)\n",
    "\n",
    "\n",
    "def create_attention_type1(input_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Create a model where the output is a linear function with learnable weights and\n",
    "    attention scores calculated by small MLPs.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input data.\n",
    "\n",
    "    Returns:\n",
    "        Model: A Keras Model for the specified attention mechanism.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x1, x2 = inputs[:, 0], inputs[:, 1]\n",
    "\n",
    "    # Define the MLPs for attention with one hidden layer of 3 neurons\n",
    "    a1_hidden = Dense(3, activation='relu')(inputs)\n",
    "    a1 = Dense(1, activation='relu')(a1_hidden)\n",
    "    a2_hidden = Dense(3, activation='relu')(inputs)\n",
    "    a2 = Dense(1, activation='relu')(a2_hidden)\n",
    "\n",
    "    # Calculate y\n",
    "    w0 = tf.Variable(0.0)\n",
    "    w1 = tf.Variable(1.0)\n",
    "    w2 = tf.Variable(1.0)\n",
    "\n",
    "    y = w0 + w1 * a1 * x1 + w2 * a2 * x2\n",
    "    model = Model(inputs, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_attention_type2(input_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Create a model where the output is a weighted sum of inputs,\n",
    "    with attention scores calculated by small MLPs.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input data.\n",
    "\n",
    "    Returns:\n",
    "        Model: A Keras Model for the specified attention mechanism.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x1, x2 = inputs[:, 0], inputs[:, 1]\n",
    "\n",
    "    # Define the MLPs for attention with one hidden layer of 3 neurons\n",
    "    a1_hidden = Dense(3, activation='relu')(inputs)\n",
    "    a1 = Dense(1, activation='relu')(a1_hidden)\n",
    "    a2_hidden = Dense(3, activation='relu')(inputs)\n",
    "    a2 = Dense(1, activation='relu')(a2_hidden)\n",
    "\n",
    "    # Calculate y\n",
    "    y = a1 * x1 + a2 * x2\n",
    "    model = Model(inputs, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_attention_type3(input_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Create a model where the output is a linear function with learnable weights plus\n",
    "    attention scores calculated by small MLPs.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input data.\n",
    "\n",
    "    Returns:\n",
    "        Model: A Keras Model for the specified attention mechanism.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x1, x2 = inputs[:, 0], inputs[:, 1]\n",
    "\n",
    "    # Define the MLPs for attention with one hidden layer of 3 neurons\n",
    "    a1_hidden = Dense(3, activation='relu')(inputs)\n",
    "    a1 = Dense(1, activation='relu')(a1_hidden)\n",
    "    a2_hidden = Dense(3, activation='relu')(inputs)\n",
    "    a2 = Dense(1, activation='relu')(a2_hidden)\n",
    "\n",
    "    # Calculate y\n",
    "    w0 = tf.Variable(0.0)\n",
    "    w1 = tf.Variable(1.0)\n",
    "    w2 = tf.Variable(1.0)\n",
    "\n",
    "    y = w0 + (w1 + a1) * x1 + (w2 + a2) * x2\n",
    "    model = Model(inputs, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_attention_type4(input_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Create a model where the output is a linear function with learnable weights and\n",
    "    attention scores calculated by small MLPs with softmax activation.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input data.\n",
    "\n",
    "    Returns:\n",
    "        Model: A Keras Model for the specified attention mechanism.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x1, x2 = inputs[:, 0], inputs[:, 1]\n",
    "\n",
    "    # Define the MLPs for attention with one hidden layer of 3 neurons\n",
    "    a1_hidden = Dense(3, activation='relu')(inputs)\n",
    "    a1 = Dense(1, activation='softmax')(a1_hidden)\n",
    "    a2_hidden = Dense(3, activation='relu')(inputs)\n",
    "    a2 = Dense(1, activation='softmax')(a2_hidden)\n",
    "\n",
    "    # Calculate y\n",
    "    w0 = tf.Variable(0.0)\n",
    "    w1 = tf.Variable(1.0)\n",
    "    w2 = tf.Variable(1.0)\n",
    "\n",
    "    y = w0 + w1 * a1 * x1 + w2 * a2 * x2\n",
    "    model = Model(inputs, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_attention_type5(input_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Create a model where the output is a linear function with learnable weights and\n",
    "    attention scores calculated by small MLPs with sigmoid activation.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input data.\n",
    "\n",
    "    Returns:\n",
    "        Model: A Keras Model for the specified attention mechanism.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x1, x2 = inputs[:, 0], inputs[:, 1]\n",
    "\n",
    "    # Define the MLPs for attention with one hidden layer of 3 neurons\n",
    "    a1_hidden = Dense(3, activation='relu')(inputs)\n",
    "    a1 = Dense(1, activation='sigmoid')(a1_hidden)\n",
    "    a2_hidden = Dense(3, activation='relu')(inputs)\n",
    "    a2 = Dense(1, activation='sigmoid')(a2_hidden)\n",
    "\n",
    "    # Calculate y\n",
    "    w0 = tf.Variable(0.0)\n",
    "    w1 = tf.Variable(1.0)\n",
    "    w2 = tf.Variable(1.0)\n",
    "\n",
    "    y = w0 + w1 * a1 * x1 + w2 * a2 * x2\n",
    "    model = Model(inputs, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_attention_type6(input_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Create a model where the output is a linear function with learnable weights and\n",
    "    attention scores calculated by small MLPs with sigmoid activation,\n",
    "    normalized to sum to 1.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input data.\n",
    "\n",
    "    Returns:\n",
    "        Model: A Keras Model for the specified attention mechanism.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x1, x2 = inputs[:, 0], inputs[:, 1]\n",
    "\n",
    "    # Define the MLPs for attention with one hidden layer of 3 neurons\n",
    "    a1_hidden = Dense(3, activation='relu')(inputs)\n",
    "    a1 = Dense(1, activation='sigmoid')(a1_hidden)\n",
    "    a2_hidden = Dense(3, activation='relu')(inputs)\n",
    "    a2 = Dense(1, activation='sigmoid')(a2_hidden)\n",
    "\n",
    "    # Normalize attention scores\n",
    "    sum_attentions = a1 + a2\n",
    "    a1 = Lambda(lambda x: x / sum_attentions)(a1)\n",
    "    a2 = Lambda(lambda x: x / sum_attentions)(a2)\n",
    "\n",
    "    # Calculate y\n",
    "    w0 = tf.Variable(0.0)\n",
    "    w1 = tf.Variable(1.0)\n",
    "    w2 = tf.Variable(1.0)\n",
    "\n",
    "    y = w0 + w1 * a1 * x1 + w2 * a2 * x2\n",
    "    model = Model(inputs, y)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_print_results(model: Model, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray,\n",
    "                            y_test: np.ndarray, epochs: int = 100) -> None:\n",
    "    \"\"\"\n",
    "    Train the model and print the results, including learned weights and attention scores.\n",
    "\n",
    "    Args:\n",
    "        model (Model): The Keras model to train.\n",
    "        x_train (np.ndarray): Training features.\n",
    "        y_train (np.ndarray): Training labels.\n",
    "        x_test (np.ndarray): Test features.\n",
    "        y_test (np.ndarray): Test labels.\n",
    "        epochs (int): Number of epochs to train the model.\n",
    "    \"\"\"\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(x_train, y_train, epochs=epochs, verbose=0, validation_data=(x_test, y_test))\n",
    "\n",
    "    # Print learned weights and attention scores\n",
    "    for layer in model.layers:\n",
    "        if 'dense' in layer.name:\n",
    "            print(f\"Layer {layer.name} weights: {layer.get_weights()}\")\n",
    "        elif 'variable' in layer.name:\n",
    "            print(f\"Layer {layer.name} variable: {layer.numpy()}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Test loss: {loss}\")\n",
    "\n",
    "\n",
    "# Training and printing results for each attention type\n",
    "input_shape = (2,)\n",
    "models = [\n",
    "    create_attention_type1(input_shape),\n",
    "    create_attention_type2(input_shape),\n",
    "    create_attention_type3(input_shape),\n",
    "    create_attention_type4(input_shape),\n",
    "    create_attention_type5(input_shape),\n",
    "    create_attention_type6(input_shape),\n",
    "]\n",
    "\n",
    "for i, model in enumerate(models, start=1):\n",
    "    print(f\"\\nAttention Type {i}\")\n",
    "    train_and_print_results(model, x_train, y_train, x_test, y_test)\n"
   ],
   "id": "8f8b9f32ea606ddf",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
