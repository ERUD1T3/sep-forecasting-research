{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME Dataset Preprocessing\n",
    "\n",
    "## Stratified Sampling Strategy\n",
    "\n",
    "This notebook preprocesses the CME dataset (CSV) using stratified sampling to create:\n",
    "\n",
    "1. **Training Set**: 2/3 of the total rows\n",
    "2. **Test Set**: 1/3 of the total rows\n",
    "3. **Cross-Validation Folds**: 4 folds where each fold represents 1/4 of the training set\n",
    "   - **Subtraining Set**: 3/4 of the folds (75% of training data)\n",
    "   - **Validation Set**: The left-out 1/4 fold (25% of training data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP_CME_PATH = 'C:/Users/the_3/Documents/github/keras-functional-api/data/sep_cme/SEP10MeV.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2297\n",
      "Number of columns: 29\n",
      "\n",
      "--- Column Names and Statistics ---\n",
      "- 2nd_order_speed_20R       | Min: 0 | Max: 3728 | Unique: 878\n",
      "- 2nd_order_speed_final     | Min: 0 | Max: 3090 | Unique: 883\n",
      "- Accelaration              | Min: -240.1 | Max: 711.6 | Unique: 681\n",
      "- CME_CDAW_LinearSpeed      | Min: 26 | Max: 3163 | Unique: 876\n",
      "- CME_CDAW_MPA              | Min: 0 | Max: 360 | Unique: 355\n",
      "- CME_CDAW_time             | Min: 1/1/2012 13:36 | Max: 9/9/2017 23:12 | Unique: 2293\n",
      "- CME_DONKI_latitude        | Min: -88 | Max: 90 | Unique: 161\n",
      "- CME_DONKI_longitude       | Min: -180.0 | Max: 180.0 | Unique: 355\n",
      "- CME_DONKI_speed           | Min: 60 | Max: 2800 | Unique: 713\n",
      "- CME_DONKI_time            | Min: 1/1/2012 13:36 | Max: 9/9/2017 23:12 | Unique: 2297\n",
      "- CMEs_in_past_9hours       | Min: 1 | Max: 5 | Unique: 5\n",
      "- CMEs_in_past_month        | Min: 1 | Max: 78 | Unique: 78\n",
      "- CMEs_with_speed_over_1000_in_past_9hours | Min: 0 | Max: 2 | Unique: 3\n",
      "- CPA                       | Min: 0 | Max: 360 | Unique: 350\n",
      "- DONKI_half_width          | Min: 5 | Max: 92 | Unique: 71\n",
      "- Halo                      | Min: 0 | Max: 1 | Unique: 2\n",
      "- SEP_onset_time            | Min/Max: NA | Unique: 83\n",
      "- Type2_Viz_Area            | Min: 0 | Max: 49362750 | Unique: 144\n",
      "- VlogV                     | Min: 84.71050999 | Max: 25491.49069 | Unique: 876\n",
      "- connection_angle_degrees  | Min: -177.6765445 | Max: 175.0916069 | Unique: 2296\n",
      "- daily_sunspots            | Min: 0 | Max: 199 | Unique: 179\n",
      "- diffusive_shock           | Min: 6.77e-50 | Max: 2.6e-09 | Unique: 709\n",
      "- half_richardson_value     | Min: -8.536764321 | Max: -0.000573753 | Unique: 2296\n",
      "- index                     | Min: 0 | Max: 2297 | Unique: 2297\n",
      "- ln_peak_intensity         | Min: -1.609437912 | Max: 8.732078739 | Unique: 84\n",
      "- max_CME_speed_in_past_day | Min: 95 | Max: 2800 | Unique: 593\n",
      "- peak_intensity            | Min: 0.2 | Max: 6198.6 | Unique: 84\n",
      "- solar_wind_speed          | Min: 256.5 | Max: 815.0 | Unique: 1287\n",
      "- target                    | Min: 0 | Max: 1 | Unique: 2\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataframe(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Analyze a dataframe by printing its dimensions, column names, and statistics for each column.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe to analyze\n",
    "    \n",
    "    Returns:\n",
    "    - None: This function only prints information, it doesn't return any values\n",
    "    \"\"\"\n",
    "    # --- Get dataset dimensions ---\n",
    "    num_rows, num_cols = df.shape\n",
    "    print(f\"Number of rows: {num_rows}\")\n",
    "    print(f\"Number of columns: {num_cols}\")\n",
    "\n",
    "    # --- Get column names with statistics ---\n",
    "    print(\"\\n--- Column Names and Statistics ---\")\n",
    "    for col in sorted(df.columns):\n",
    "        stats = []\n",
    "        try:\n",
    "            # Attempt to get min/max - might fail for non-numeric/non-comparable types\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            stats.append(f\"Min: {min_val}\")\n",
    "            stats.append(f\"Max: {max_val}\")\n",
    "        except TypeError:\n",
    "            stats.append(\"Min/Max: NA\")\n",
    "\n",
    "        # Get unique count - works for most types\n",
    "        unique_count = df[col].nunique()\n",
    "        stats.append(f\"Unique: {unique_count}\")\n",
    "        \n",
    "        # Display column name with its statistics on the same line\n",
    "        print(f\"- {col:<25} | {' | '.join(stats)}\")\n",
    "\n",
    "\n",
    "# Load and analyze the dataset\n",
    "try:\n",
    "    # Load the dataset from the specified path\n",
    "    df = pd.read_csv(SEP_CME_PATH)\n",
    "    \n",
    "    # Analyze the loaded dataframe\n",
    "    analyze_dataframe(df)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at '{SEP_CME_PATH}'. Please provide the correct file path.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_norm(data: Union[pd.DataFrame, pd.Series]) -> Union[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Apply min-max normalization to a pandas DataFrame or Series.\n",
    "    If the min and max values of a column are the same, that column is replaced with zeros.\n",
    "\n",
    "    Parameters:\n",
    "    - cme_files (pd.DataFrame or pd.Series): The pandas DataFrame or Series to be normalized.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame or pd.Series: Min-max normalized pandas DataFrame or Series.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to normalize a single column\n",
    "    def normalize_column(column: pd.Series) -> pd.Series:\n",
    "        min_val = column.min()\n",
    "        max_val = column.max()\n",
    "\n",
    "        # Handle case where max and min are the same\n",
    "        if min_val == max_val:\n",
    "            return pd.Series(np.zeros_like(column), index=column.index)\n",
    "        else:\n",
    "            # Apply min-max normalization\n",
    "            return (column - min_val) / (max_val - min_val)\n",
    "\n",
    "    # Check if the input is a DataFrame\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        normalized_df = data.apply(normalize_column, axis=0)\n",
    "        return normalized_df\n",
    "\n",
    "    # Check if the input is a Series\n",
    "    elif isinstance(data, pd.Series):\n",
    "        return normalize_column(data)\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"Input must be a pandas DataFrame or Series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cme_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply preprocessing steps to the CME dataset features.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataframe to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The preprocessed dataframe.\n",
    "    \"\"\"\n",
    "    # Preallocate a dictionary to store preprocessed data\n",
    "    preprocessed_data = {}\n",
    "    \n",
    "    # Include ln_peak_intensity without normalization (as it will be the target)\n",
    "    preprocessed_data['ln_peak_intensity'] = df['ln_peak_intensity']\n",
    "    \n",
    "    # Process solar_wind_speed (was missing)\n",
    "    preprocessed_data['solar_wind_speed_norm'] = min_max_norm(df['solar_wind_speed'])\n",
    "    \n",
    "    # Process connection_angle_degrees (was missing)\n",
    "    preprocessed_data['connection_angle_degrees_norm'] = min_max_norm(df['connection_angle_degrees'])\n",
    "    \n",
    "    # Log transformations for specific features\n",
    "    preprocessed_data['log_half_richardson_value'] = np.log1p(-df['half_richardson_value'])\n",
    "    preprocessed_data['log_diffusive_shock'] = np.log1p(df['diffusive_shock'])\n",
    "    preprocessed_data['log_Type2_Viz_Area'] = df['Type2_Viz_Area'].apply(lambda x: np.log(x) if x != 0 else np.log(1))\n",
    "    \n",
    "    # Apply Min-Max normalization on all features, including the log-transformed ones\n",
    "    for feature, proper_name in {'VlogV': 'VlogV', \n",
    "                                'CME_DONKI_speed': 'CME_DONKI_speed',\n",
    "                                'CME_DONKI_latitude': 'CME_DONKI_latitude', \n",
    "                                'CME_DONKI_longitude': 'CME_DONKI_longitude', \n",
    "                                'CME_CDAW_MPA': 'CME_CDAW_MPA',\n",
    "                                'CME_CDAW_LinearSpeed': 'CME_CDAW_LinearSpeed',\n",
    "                                'DONKI_half_width': 'DONKI_half_width',\n",
    "                                'Accelaration': 'Accelaration',\n",
    "                                '2nd_order_speed_final': '2nd_order_speed_final',\n",
    "                                '2nd_order_speed_20R': '2nd_order_speed_20R',\n",
    "                                'CPA': 'CPA',\n",
    "                                'daily_sunspots': 'daily_sunspots',\n",
    "                                'CMEs_in_past_month': 'CMEs_in_past_month',\n",
    "                                'CMEs_in_past_9hours': 'CMEs_in_past_9hours',\n",
    "                                'CMEs_with_speed_over_1000_in_past_9hours': 'CMEs_with_speed_over_1000_in_past_9hours',\n",
    "                                'max_CME_speed_in_past_day': 'max_CME_speed_in_past_day'}.items():\n",
    "        preprocessed_data[f\"{feature}_norm\"] = min_max_norm(df[proper_name])\n",
    "    \n",
    "    # Normalize the log-transformed features\n",
    "    preprocessed_data['log_richardson_value_norm'] = min_max_norm(preprocessed_data['log_half_richardson_value'])\n",
    "    preprocessed_data['log_diffusive_shock_norm'] = min_max_norm(preprocessed_data['log_diffusive_shock'])\n",
    "    preprocessed_data['log_Type2_Viz_Area_norm'] = min_max_norm(preprocessed_data['log_Type2_Viz_Area'])\n",
    "    \n",
    "    # No transformation for 'Halo'\n",
    "    preprocessed_data['Halo'] = df['Halo']\n",
    "    \n",
    "    # Remove intermediate log-transformed columns\n",
    "    preprocessed_data.pop('log_half_richardson_value')\n",
    "    preprocessed_data.pop('log_diffusive_shock')\n",
    "    preprocessed_data.pop('log_Type2_Viz_Area')\n",
    "    \n",
    "    return pd.DataFrame(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2297\n",
      "Number of columns: 23\n",
      "\n",
      "--- Column Names and Statistics ---\n",
      "- 2nd_order_speed_20R_norm  | Min: 0.0 | Max: 1.0 | Unique: 878\n",
      "- 2nd_order_speed_final_norm | Min: 0.0 | Max: 1.0 | Unique: 883\n",
      "- Accelaration_norm         | Min: 0.0 | Max: 1.0 | Unique: 681\n",
      "- CME_CDAW_LinearSpeed_norm | Min: 0.0 | Max: 1.0 | Unique: 876\n",
      "- CME_CDAW_MPA_norm         | Min: 0.0 | Max: 1.0 | Unique: 355\n",
      "- CME_DONKI_latitude_norm   | Min: 0.0 | Max: 1.0 | Unique: 161\n",
      "- CME_DONKI_longitude_norm  | Min: 0.0 | Max: 1.0 | Unique: 355\n",
      "- CME_DONKI_speed_norm      | Min: 0.0 | Max: 1.0 | Unique: 713\n",
      "- CMEs_in_past_9hours_norm  | Min: 0.0 | Max: 1.0 | Unique: 5\n",
      "- CMEs_in_past_month_norm   | Min: 0.0 | Max: 1.0 | Unique: 78\n",
      "- CMEs_with_speed_over_1000_in_past_9hours_norm | Min: 0.0 | Max: 1.0 | Unique: 3\n",
      "- CPA_norm                  | Min: 0.0 | Max: 1.0 | Unique: 350\n",
      "- DONKI_half_width_norm     | Min: 0.0 | Max: 1.0 | Unique: 71\n",
      "- Halo                      | Min: 0 | Max: 1 | Unique: 2\n",
      "- VlogV_norm                | Min: 0.0 | Max: 1.0 | Unique: 876\n",
      "- connection_angle_degrees_norm | Min: 0.0 | Max: 1.0 | Unique: 2296\n",
      "- daily_sunspots_norm       | Min: 0.0 | Max: 1.0 | Unique: 179\n",
      "- ln_peak_intensity         | Min: -1.609437912 | Max: 8.732078739 | Unique: 84\n",
      "- log_Type2_Viz_Area_norm   | Min: 0.0 | Max: 1.0 | Unique: 144\n",
      "- log_diffusive_shock_norm  | Min: 0.0 | Max: 1.0 | Unique: 709\n",
      "- log_richardson_value_norm | Min: 0.0 | Max: 1.0 | Unique: 2296\n",
      "- max_CME_speed_in_past_day_norm | Min: 0.0 | Max: 1.0 | Unique: 593\n",
      "- solar_wind_speed_norm     | Min: 0.0 | Max: 1.0 | Unique: 1287\n"
     ]
    }
   ],
   "source": [
    "# preprocess the dataset\n",
    "preprocessed_df = preprocess_cme_features(df)\n",
    "\n",
    "# print the statistics of the preprocessed dataframe\n",
    "analyze_dataframe(preprocessed_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to: C:/Users/the_3/Documents/github/keras-functional-api/data/sep_cme\\SEP10MeV_preprocessed.csv\n",
      "\n",
      "Columns in the saved file (last one is the target):\n",
      "solar_wind_speed_norm, connection_angle_degrees_norm, VlogV_norm, CME_DONKI_speed_norm, CME_DONKI_latitude_norm, CME_DONKI_longitude_norm, CME_CDAW_MPA_norm, CME_CDAW_LinearSpeed_norm, DONKI_half_width_norm, Accelaration_norm, 2nd_order_speed_final_norm, 2nd_order_speed_20R_norm, CPA_norm, daily_sunspots_norm, CMEs_in_past_month_norm, CMEs_in_past_9hours_norm, CMEs_with_speed_over_1000_in_past_9hours_norm, max_CME_speed_in_past_day_norm, log_richardson_value_norm, log_diffusive_shock_norm, log_Type2_Viz_Area_norm, Halo, ln_peak_intensity\n"
     ]
    }
   ],
   "source": [
    "# Rearrange columns to place ln_peak_intensity last\n",
    "cols = [col for col in preprocessed_df.columns if col != 'ln_peak_intensity']\n",
    "cols.append('ln_peak_intensity')\n",
    "preprocessed_df = preprocessed_df[cols]\n",
    "\n",
    "# Define the output path in the same directory as the original file\n",
    "import os\n",
    "output_dir = os.path.dirname(SEP_CME_PATH)\n",
    "output_path = os.path.join(output_dir, 'SEP10MeV_preprocessed.csv')\n",
    "\n",
    "# Save to CSV\n",
    "preprocessed_df.to_csv(output_path, index=False)\n",
    "print(f\"Preprocessed data saved to: {output_path}\")\n",
    "\n",
    "# Verify the column order in the saved file\n",
    "print(\"\\nColumns in the saved file (last one is the target):\")\n",
    "print(', '.join(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded preprocessed data from: C:/Users/the_3/Documents/github/keras-functional-api/data/sep_cme\\SEP10MeV_preprocessed.csv\n",
      "Shape of loaded data: (2297, 23)\n",
      "\n",
      "Analysis of the loaded preprocessed data:\n",
      "Number of rows: 2297\n",
      "Number of columns: 23\n",
      "\n",
      "--- Column Names and Statistics ---\n",
      "- 2nd_order_speed_20R_norm  | Min: 0.0 | Max: 1.0 | Unique: 878\n",
      "- 2nd_order_speed_final_norm | Min: 0.0 | Max: 1.0 | Unique: 883\n",
      "- Accelaration_norm         | Min: 0.0 | Max: 1.0 | Unique: 681\n",
      "- CME_CDAW_LinearSpeed_norm | Min: 0.0 | Max: 1.0 | Unique: 876\n",
      "- CME_CDAW_MPA_norm         | Min: 0.0 | Max: 1.0 | Unique: 355\n",
      "- CME_DONKI_latitude_norm   | Min: 0.0 | Max: 1.0 | Unique: 161\n",
      "- CME_DONKI_longitude_norm  | Min: 0.0 | Max: 1.0 | Unique: 355\n",
      "- CME_DONKI_speed_norm      | Min: 0.0 | Max: 1.0 | Unique: 713\n",
      "- CMEs_in_past_9hours_norm  | Min: 0.0 | Max: 1.0 | Unique: 5\n",
      "- CMEs_in_past_month_norm   | Min: 0.0 | Max: 1.0 | Unique: 78\n",
      "- CMEs_with_speed_over_1000_in_past_9hours_norm | Min: 0.0 | Max: 1.0 | Unique: 3\n",
      "- CPA_norm                  | Min: 0.0 | Max: 1.0 | Unique: 350\n",
      "- DONKI_half_width_norm     | Min: 0.0 | Max: 1.0 | Unique: 71\n",
      "- Halo                      | Min: 0 | Max: 1 | Unique: 2\n",
      "- VlogV_norm                | Min: 0.0 | Max: 1.0 | Unique: 876\n",
      "- connection_angle_degrees_norm | Min: 0.0 | Max: 1.0 | Unique: 2296\n",
      "- daily_sunspots_norm       | Min: 0.0 | Max: 1.0 | Unique: 179\n",
      "- ln_peak_intensity         | Min: -1.609437912 | Max: 8.732078739 | Unique: 84\n",
      "- log_Type2_Viz_Area_norm   | Min: 0.0 | Max: 1.0 | Unique: 144\n",
      "- log_diffusive_shock_norm  | Min: 0.0 | Max: 1.0 | Unique: 709\n",
      "- log_richardson_value_norm | Min: 0.0 | Max: 1.0 | Unique: 2296\n",
      "- max_CME_speed_in_past_day_norm | Min: 0.0 | Max: 1.0 | Unique: 593\n",
      "- solar_wind_speed_norm     | Min: 0.0 | Max: 1.0 | Unique: 1287\n",
      "\n",
      "Verifying target column position:\n",
      "Last column in loaded data: ln_peak_intensity\n",
      "✓ Target column 'ln_peak_intensity' is correctly positioned as the last column\n"
     ]
    }
   ],
   "source": [
    "# Load the saved preprocessed CSV file to verify it was saved correctly\n",
    "loaded_df = pd.read_csv(output_path)\n",
    "print(f\"\\nLoaded preprocessed data from: {output_path}\")\n",
    "print(f\"Shape of loaded data: {loaded_df.shape}\")\n",
    "\n",
    "# Analyze the loaded dataframe to verify preprocessing was successful\n",
    "print(\"\\nAnalysis of the loaded preprocessed data:\")\n",
    "analyze_dataframe(loaded_df)\n",
    "\n",
    "# Verify that the target column is the last column\n",
    "print(\"\\nVerifying target column position:\")\n",
    "print(f\"Last column in loaded data: {loaded_df.columns[-1]}\")\n",
    "if loaded_df.columns[-1] == 'ln_peak_intensity':\n",
    "    print(\"✓ Target column 'ln_peak_intensity' is correctly positioned as the last column\")\n",
    "else:\n",
    "    print(\"✗ Target column is not in the expected position\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        seed: int = None,\n",
    "        shuffle: bool = True,\n",
    "        debug: bool = False\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into subtraining and validation sets using stratified sampling.\n",
    "    The validation is a quarter of the dataset, and the rest is used for subtraining.\n",
    "\n",
    "    Parameters:\n",
    "    X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n",
    "    y (np.ndarray): Label vector of shape (n_samples, 1).\n",
    "    shuffle (bool): Whether to shuffle the data before splitting. Default is True.\n",
    "    seed (int): Random seed for reproducibility. Default is None.\n",
    "    debug (bool): Whether to plot the distributions of the original, subtrain, and validation sets. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: Split feature and label matrices:\n",
    "        - X_subtrain: Features for the subtraining set.\n",
    "        - y_subtrain: Labels for the subtraining set.\n",
    "        - X_val: Features for the validation set.\n",
    "        - y_val: Labels for the validation set.\n",
    "    \"\"\"\n",
    "    if shuffle: np.random.seed(seed)\n",
    "    # Sort the data by the labels\n",
    "    sorted_indices = np.argsort(y, axis=0).flatten()\n",
    "    X_sorted, y_sorted = X[sorted_indices], y[sorted_indices]\n",
    "    # Calculate the number of validation samples\n",
    "    num_samples = X.shape[0]\n",
    "    # val_size = int(num_samples * split)\n",
    "    # Initialize lists to hold subtraining and validation data\n",
    "    X_subtrain, y_subtrain, X_val, y_val = [], [], [], []\n",
    "    # Divide into groups of 4 and split into subtrain and validation\n",
    "    for i in range(0, num_samples, 4):\n",
    "        group_indices = list(range(i, min(i + 4, num_samples)))\n",
    "        if shuffle: np.random.shuffle(group_indices)  # Shuffle within the group\n",
    "        val_indices = group_indices[:1]\n",
    "        subtrain_indices = group_indices[1:]\n",
    "        # Append the samples to the subtraining and validation sets\n",
    "        X_val.extend(X_sorted[val_indices])\n",
    "        y_val.extend(y_sorted[val_indices])\n",
    "        X_subtrain.extend(X_sorted[subtrain_indices])\n",
    "        y_subtrain.extend(y_sorted[subtrain_indices])\n",
    "\n",
    "    # Convert lists back to arrays\n",
    "    X_subtrain, y_subtrain = np.array(X_subtrain), np.array(y_subtrain)\n",
    "    X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "\n",
    "    # Ensure the largest y is in the validation set\n",
    "    max_y_index = np.argmax(y_sorted)  # Index of the largest y value\n",
    "    max_y_val = y_sorted[max_y_index]  # Largest y value\n",
    "    # Check if the largest y value is not in the validation set\n",
    "    if max_y_val not in y_val:\n",
    "        # Add the sample with the largest y value to the validation set\n",
    "        X_val = np.vstack([X_val, X_sorted[max_y_index].reshape(1, -1)])\n",
    "        y_val = np.vstack([y_val, max_y_val.reshape(1, -1)])\n",
    "        # # Remove the largest y from the subtraining set\n",
    "        # mask = y_subtrain != max_y_val\n",
    "        # X_subtrain = X_subtrain[mask.flatten()]\n",
    "        # y_subtrain = y_subtrain[mask.flatten()]\n",
    "\n",
    "    if debug:\n",
    "        plot_distributions(y, y_subtrain, y_val)\n",
    "\n",
    "    return X_subtrain, y_subtrain, X_val, y_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
